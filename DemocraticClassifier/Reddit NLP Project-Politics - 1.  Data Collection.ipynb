{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement:\n",
    "\n",
    "Given the common ground of the 2020 presidential nominees for the Democratic Party, would it be possible to use Classification modeling and Natural Language Processing techniques, to take a random headline and body text from a candidate's Subreddit and determine which subreddit it came from.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Summary\n",
    "\n",
    "After a painstaking attempt to use classification modeling techniques (excluding neural networks), it was determined to be very difficult to classify the 4 thousand+ documents collected from the subreddits of Bernie Sanders, Elizabeth Warren, Kamala Harris, and Pete Buttigieg.  However, three benchmarks are worth noting right off the top.\n",
    "\n",
    "First, the baseline accuracy was only 26%, as we were training the models on four different classes.  There was also one unbalanced class, as I was not able to collect quite as many posts from the Elizabeth Warren Subreddit.\n",
    "\n",
    "Second, the accuracy was actually quite high (close to 90%) when the candidate names / nicknames were left in the model.  However, I found it more interesting to try to create a model that excluded these, in order to see if there are any strong cultural or policy differentiators between the candidates.\n",
    "\n",
    "And third, while the model that took these considerations under advisement performed with an accuracy of only 52%, the is still double the baseline score, and it provided us with a handfull of insights about how the candidates are within the Reddit bubble.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import requests\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximize DataFrame Display Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the max column setting in pandas\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a 'User-Agent header' for Access to Reddit API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent' : 'Calliope'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to Scrape Reddit API and Convert to DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  Subreddit Scraper Instantiator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function to scrape a subreddit, with an output list name, a website slug, and a number of iterations.  \n",
    "\n",
    "def subredditscraper_instantiate(slug):\n",
    "\n",
    "    output_list = []\n",
    "\n",
    "    params = {'after' : None}\n",
    "    url = 'https://www.reddit.com/r/'+slug+'.json'\n",
    "    res = requests.get(url, params=params, headers=headers)\n",
    "    if res.status_code == 200:\n",
    "        slug_json = res.json()\n",
    "        output_list.extend(slug_json['data']['children'])\n",
    "        after = slug_json['data']['after']\n",
    "        #print (after)\n",
    "    else:\n",
    "        print(res.status_code)\n",
    "    time.sleep(1)\n",
    "    return (output_list)\n",
    "\n",
    "    #Confirm unique posts    \n",
    "    #len(set([p['data']['name'] for p in output_list]))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  Subreddit Scraper Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subredditscraper_iterator(slug, redd_list=False, iterations=1):     \n",
    "    for i in range(iterations):\n",
    "        if redd_list == False:\n",
    "            params = {}\n",
    "            red_list = subredditscraper_instantiate(slug)\n",
    "        else:\n",
    "            after = redd_list[-1]['data']['name']\n",
    "            params = {'after' : after,\n",
    "                     \"default_comment_sort\" : 'new'}\n",
    "        url = 'https://www.reddit.com/r/'+slug+'.json'\n",
    "        res = requests.get(url, params=params, headers=headers)\n",
    "        if res.status_code == 200:\n",
    "            slug_json = res.json()\n",
    "            redd_list.extend(slug_json['data']['children'])\n",
    "            after = slug_json['data']['after']\n",
    "            #print (after)\n",
    "        else:\n",
    "            print(res.status_code)\n",
    "            break\n",
    "        time.sleep(1)\n",
    "    print(len(set([p['data']['name'] for p in redd_list])))    \n",
    "    return redd_list\n",
    "        \n",
    "        \n",
    "    \n",
    "#Confirm unique posts    \n",
    "#len(set([p['data']['name'] for p in output_list]))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  List to Pandas Converter\n",
    "#### Drop Duplicates and Convert to Pandas DataFrame, with optional save to .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reddit_to_df(redd_list, output_csv = False):\n",
    "    #Convert to DataFrame and drop duplicates\n",
    "\n",
    "    output_df = pd.DataFrame(redd_list)\n",
    "    output_df = pd.DataFrame([posts for posts in output_df['data']])\n",
    "    output_df.drop_duplicates('name', keep = 'first', inplace = True)\n",
    "    if output_csv:    \n",
    "        output_df.to_csv('./datasets/'+output_csv)\n",
    "    return(output_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit URL Slugs for Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Democratic Primary Competitors\n",
    "\n",
    "#Bernie Sanders\n",
    "bernie_slug = 'SandersForPresident'\n",
    "\n",
    "#Pete Buttigieg\n",
    "butti_slug = 'Pete_Buttigieg'\n",
    "\n",
    "#Kamala Harris\n",
    "kamala_slug = 'Kamala'\n",
    "\n",
    "#Elizabeth Warren\n",
    "warren_slug = 'ElizabethWarren'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparison Slugs\n",
    "\n",
    "climate_slug = 'ClimateOffensive'\n",
    "funny_slug = 'funny'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate Reddit Lists "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ----Run Only Once----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test subreddit\n",
    "if 0==1:\n",
    "    funny_list = subredditscraper_instantiate('funny')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparison Subreddit for Democratic Primary Competitors\n",
    "if 0==1:\n",
    "    climate_list = subredditscraper_instantiate('ClimateOffensive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Democratic Primary Competitors\n",
    "if 0==1:\n",
    "    bernie_list = subredditscraper_instantiate(bernie_slug) \n",
    "    butti_list = subredditscraper_instantiate(butti_slug)\n",
    "    kamala_list = subredditscraper_instantiate(kamala_slug)\n",
    "    warren_list = subredditscraper_instantiate(warren_slug)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate Reddit Lists\n",
    "#### Come back to this section to generate more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "760\n",
      "972\n"
     ]
    }
   ],
   "source": [
    "#Comparison Subreddits\n",
    "\n",
    "funny_list = subredditscraper_iterator('funny', funny_list, 40)\n",
    "climate_list = subredditscraper_iterator('ClimateOffensive', climate_list, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "990\n",
      "988\n",
      "997\n",
      "797\n"
     ]
    }
   ],
   "source": [
    "#Democratic Primary Competitors\n",
    "\n",
    "bernie_list = subredditscraper_iterator('SandersForPresident', bernie_list, 40) \n",
    "butti_list = subredditscraper_iterator('Pete_Buttigieg', butti_list, 40)\n",
    "kamala_list = subredditscraper_iterator('Kamala', kamala_list, 40)\n",
    "warren_list = subredditscraper_iterator('ElizabethWarren', warren_list , 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to DataFrames and Save to .CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparison Subreddits\n",
    "\n",
    "funny_df = reddit_to_df(funny_list, 'raw_funny.csv')\n",
    "climate_df = reddit_to_df(climate_list, 'raw_climate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Democratic Candidates\n",
    "\n",
    "bernie_df = reddit_to_df(bernie_list, 'raw_bernie.csv')\n",
    "butti_df = reddit_to_df(butti_list, 'raw_butti.csv')\n",
    "kamala_df = reddit_to_df(kamala_list, 'raw_kamala.csv')\n",
    "warren_df = reddit_to_df(warren_list, 'raw_warren.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
