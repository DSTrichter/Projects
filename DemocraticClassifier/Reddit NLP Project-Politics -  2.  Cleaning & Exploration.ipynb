{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import requests\n",
    "import time\n",
    "\n",
    "#For Modeling\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximize DataFrame Display Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the max column setting in pandas\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load .CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "funny_df = pd.read_csv('./datasets/raw_funny.csv')\n",
    "climate_df = pd.read_csv('./datasets/raw_climate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernie_df = pd.read_csv('./datasets/raw_bernie.csv')\n",
    "butti_df = pd.read_csv('./datasets/raw_butti.csv')\n",
    "kamala_df = pd.read_csv('./datasets/raw_kamala.csv')\n",
    "warren_df = pd.read_csv('./datasets/raw_warren.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare for EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidtrichter/anaconda3/envs/dsi/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/davidtrichter/anaconda3/envs/dsi/lib/python3.7/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/Users/davidtrichter/anaconda3/envs/dsi/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Users/davidtrichter/anaconda3/envs/dsi/lib/python3.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "#Create modified DataFrames for the test Subreddits\n",
    "new_bernie_df = bernie_df[['title', 'selftext']]\n",
    "new_butti_df = butti_df[['title', 'selftext']]\n",
    "new_kamala_df = kamala_df[['title', 'selftext']]\n",
    "new_warren_df = warren_df[['title', 'selftext']]\n",
    "\n",
    "#Create a new 'label' column that will be mapped to our target (y) variable\n",
    "new_bernie_df['label'] = 'bernie'\n",
    "new_butti_df['label'] = 'buttigieg'\n",
    "new_kamala_df['label'] = 'kamala'\n",
    "new_warren_df['label'] = 'warren'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine dataframes into a single list for concatenation\n",
    "\n",
    "list_of_dfs = [new_bernie_df, new_butti_df, new_kamala_df, new_warren_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in list_of_dfs:\n",
    "    df = df['selftext'].fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine the Bernie and Buttigieg DataFrames\n",
    "\n",
    "df = pd.concat(list_of_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a combined text column, 'all_text'\n",
    "df['all_text'] = df['title'] + ' ' + df['selftext']\n",
    "\n",
    "#Drop 'title' and 'body text' columns, leaving only the combined column\n",
    "#df = df.drop(['selftext'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kamala       0.264316\n",
       "bernie       0.262460\n",
       "buttigieg    0.261930\n",
       "warren       0.211294\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>label</th>\n",
       "      <th>all_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Canvassing update! Where is everyone at now?</td>\n",
       "      <td></td>\n",
       "      <td>bernie</td>\n",
       "      <td>Canvassing update! Where is everyone at now?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Colorized picture of Bernie Sanders being arre...</td>\n",
       "      <td></td>\n",
       "      <td>bernie</td>\n",
       "      <td>Colorized picture of Bernie Sanders being arre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bernie Sanders had the best quote at the debate</td>\n",
       "      <td></td>\n",
       "      <td>bernie</td>\n",
       "      <td>Bernie Sanders had the best quote at the debate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bernie: DC is flooded. It was 90 degrees in Al...</td>\n",
       "      <td></td>\n",
       "      <td>bernie</td>\n",
       "      <td>Bernie: DC is flooded. It was 90 degrees in Al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sanders and Ocasio-Cortez move to declare clim...</td>\n",
       "      <td></td>\n",
       "      <td>bernie</td>\n",
       "      <td>Sanders and Ocasio-Cortez move to declare clim...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title selftext   label  \\\n",
       "0       Canvassing update! Where is everyone at now?           bernie   \n",
       "1  Colorized picture of Bernie Sanders being arre...           bernie   \n",
       "2    Bernie Sanders had the best quote at the debate           bernie   \n",
       "3  Bernie: DC is flooded. It was 90 degrees in Al...           bernie   \n",
       "4  Sanders and Ocasio-Cortez move to declare clim...           bernie   \n",
       "\n",
       "                                            all_text  \n",
       "0      Canvassing update! Where is everyone at now?   \n",
       "1  Colorized picture of Bernie Sanders being arre...  \n",
       "2   Bernie Sanders had the best quote at the debate   \n",
       "3  Bernie: DC is flooded. It was 90 degrees in Al...  \n",
       "4  Sanders and Ocasio-Cortez move to declare clim...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title       0\n",
       "selftext    0\n",
       "label       0\n",
       "all_text    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./datasets/reddit_politic.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Features and Target Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['label'].map({'bernie':0, 'buttigieg':1, 'kamala': 2, 'warren': 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import SpaCy and Instantiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Spacy Stopwords and Custom Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are: 326 built-in stop words\n"
     ]
    }
   ],
   "source": [
    "#import SpaCy stopwords\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "print(f'There are: {len(spacy_stopwords)} built-in stop words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are: 10 built-in + custom stop words\n"
     ]
    }
   ],
   "source": [
    "#Create custom list of stop words\n",
    "reddit_stopwords = set()\n",
    "\n",
    "reddit_stopwords.update(['Buttigieg','Pete','Bernie','Bern','Sanders','Mayor','Elizabeth', 'Warren', 'Kamala', 'Harris'])\n",
    "\n",
    "\n",
    "print(f'There are: {len(reddit_stopwords)} built-in + custom stop words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a train and test dataframes that includes both X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate DataFrames\n",
    "train_df = pd.DataFrame()\n",
    "test_df = pd.DataFrame()\n",
    "\n",
    "train_df['text'] = X_train\n",
    "train_df['cat'] = y_train\n",
    "\n",
    "test_df['text'] = X_test\n",
    "test_df['cat'] = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up train dataframe tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Can anyone help me find the interview where Pete describes his first date with Chasten?',\n",
       "  1)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['tuples'] = train_df.apply(\n",
    "    lambda row: (row['text'],row['cat']), axis=1)\n",
    "train = train_df['tuples'].tolist()\n",
    "train[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up train dataframe tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pete speaking after being denied entry to a child detention center in Florida this morning. \"No more child prisons!\"',\n",
       "  1)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['tuples'] = test_df.apply(\n",
    "    lambda row: (row['text'],row['cat']), axis=1)\n",
    "test = test_df['tuples'].tolist()\n",
    "test[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions from spacy documentation\n",
    "def load_data(limit=0, split=0.8):\n",
    "    train_data = train\n",
    "    np.random.shuffle(train_data)\n",
    "    train_data = train_data[-limit:]\n",
    "    texts, labels = zip(*train_data)\n",
    "    cats = [{'POSITIVE': bool(y)} for y in labels]\n",
    "    split = int(len(train_data) * split)\n",
    "    return (texts[:split], cats[:split]), (texts[split:], cats[split:])\n",
    "\n",
    "def evaluate(tokenizer, textcat, texts, cats):\n",
    "    docs = (tokenizer(text) for text in texts)\n",
    "    tp = 1e-8  # True positives\n",
    "    fp = 1e-8  # False positives\n",
    "    fn = 1e-8  # False negatives\n",
    "    tn = 1e-8  # True negatives\n",
    "    for i, doc in enumerate(textcat.pipe(docs)):\n",
    "        gold = cats[i]\n",
    "        for label, score in doc.cats.items():\n",
    "            if label not in gold:\n",
    "                continue\n",
    "            if score >= 0.5 and gold[label] >= 0.5:\n",
    "                tp += 1.\n",
    "            elif score >= 0.5 and gold[label] < 0.5:\n",
    "                fp += 1.\n",
    "            elif score < 0.5 and gold[label] < 0.5:\n",
    "                tn += 1\n",
    "            elif score < 0.5 and gold[label] >= 0.5:\n",
    "                fn += 1\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return {'textcat_p': precision, 'textcat_r': recall, 'textcat_f': f_score}\n",
    "\n",
    "#(\"Number of texts to train from\",\"t\" , int)\n",
    "n_texts=30000\n",
    "#You can increase texts count if you have more computational power.\n",
    "\n",
    "#(\"Number of training iterations\", \"n\", int))\n",
    "n_iter=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading food reviews data...\n",
      "Using 30000 examples (2112 training, 528 evaluation)\n"
     ]
    }
   ],
   "source": [
    "# add the text classifier to the pipeline if it doesn't exist\n",
    "# nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "if 'textcat' not in nlp.pipe_names:\n",
    "    textcat = nlp.create_pipe('textcat')\n",
    "    nlp.add_pipe(textcat, last=True)\n",
    "# otherwise, get it, so we can add labels to it\n",
    "else:\n",
    "    textcat = nlp.get_pipe('textcat')\n",
    "\n",
    "# add label to text classifier\n",
    "textcat.add_label('POSITIVE')\n",
    "\n",
    "# load the dataset\n",
    "print(\"Loading data...\")\n",
    "(train_texts, train_cats), (dev_texts, dev_cats) = load_data(limit=n_texts)\n",
    "print(\"Using {} examples ({} training, {} evaluation)\"\n",
    "      .format(n_texts, len(train_texts), len(dev_texts)))\n",
    "train_data = list(zip(train_texts,\n",
    "                      [{'cats': cats} for cats in train_cats]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
